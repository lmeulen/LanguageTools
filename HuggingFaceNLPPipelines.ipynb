{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "7i-zyp65LI37"
      },
      "id": "7i-zyp65LI37"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install packages"
      ],
      "metadata": {
        "id": "yWgbTCdkLREH"
      },
      "id": "yWgbTCdkLREH"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers[sentencepiece]\n",
        "!pip install sacremoses"
      ],
      "metadata": {
        "id": "SRHL-01ULTsO"
      },
      "id": "SRHL-01ULTsO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load required packages"
      ],
      "metadata": {
        "id": "M4QI6Le6Kcan"
      },
      "id": "M4QI6Le6Kcan"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "iFiqIuyGKOah"
      },
      "id": "iFiqIuyGKOah",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP tasks\n",
        "\n",
        "Generic form:\n",
        "```\n",
        "pipe = pipline(task=<task>, model=<model>)\n",
        "def perform_action(pipe, prompt):\n",
        "    print(f\"\\nPrompt : {prompt}\")\n",
        "    answer = pipe(prompt)\n",
        "    print(f\"Answer : {answer['label']}\")\n",
        "    print(f\"Score  : {answer['score']}\")\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "DjLogMKfLphe"
      },
      "id": "DjLogMKfLphe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis"
      ],
      "metadata": {
        "id": "I9TbCwUYKuKb"
      },
      "id": "I9TbCwUYKuKb"
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(task=\"sentiment-analysis\", \n",
        "                model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
        "\n",
        "def analyse_sentiment(pipe, prompt):\n",
        "    print(f\"\\nPrompt : {prompt}\")\n",
        "    answer = pipe(prompt)\n",
        "    print(f\"Answer : {answer[0]['label']}\")\n",
        "    print(f\"Score  : {answer[0]['score']}\")\n",
        "\n",
        "analyse_sentiment(pipe, \"This restaurant is awesome\")\n",
        "analyse_sentiment(pipe, \"This restaurant is bad\")"
      ],
      "metadata": {
        "id": "ixWJsXlSKjHG"
      },
      "id": "ixWJsXlSKjHG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text summarization"
      ],
      "metadata": {
        "id": "4E6RK5EyNrRs"
      },
      "id": "4E6RK5EyNrRs"
    },
    {
      "cell_type": "code",
      "source": [
        "context  = r\"\"\"\n",
        "The Mars Orbiter Mission (MOM), also called Mangalyaan (\"Mars-craft\", from\n",
        "Mangala, \"Mars\" and yāna, \"craft, vehicle\") is a space probe orbiting Mars since\n",
        "24 September 2014? It was launched on 5 November 2013 by the Indian Space\n",
        "Research Organisation (ISRO). It is India's first interplanetary mission and it\n",
        "made India the fourth country to achieve Mars orbit, after Roscosmos, NASA, and\n",
        "the European Space Company. and it made India the first country to achieve this\n",
        "in the first attempt. The Mars Orbiter took off from the First Launch Pad at\n",
        "Satish Dhawan Space Centre (Sriharikota Range SHAR), Andhra Pradesh, using a\n",
        "Polar Satellite Launch Vehicle (PSLV) rocket C25 at 09:08 UTC on 5 November\n",
        "2013. The launch window was approximately 20 days long and started on 28 October\n",
        "2013. The MOM probe spent about 36 days in  Earth orbit, where it made a series\n",
        "of seven apogee-raising orbital maneuvers before trans-Mars injection on 30 \n",
        "November 2013 (UTC).[23] After a 298-day long journey to Mars orbit, it was put\n",
        "into Mars orbit on 24 September 2014.\n",
        "\"\"\"\n",
        "\n",
        "pipe = pipeline(task=\"summarization\", \n",
        "                model=\"t5-base\", tokenizer=\"t5-base\", framework=\"tf\")\n",
        "\n",
        "def summarize(pipe, context, max_length=130, min_length=60):\n",
        "    print(f\"\\nContext :\\n{context}\")\n",
        "    summary = pipe(context, max_length=max_length, min_length=min_length)\n",
        "    print(f\"Summary : \\n{summary[0]['summary_text']}\")\n",
        "\n",
        "summarize(pipe, context)\n"
      ],
      "metadata": {
        "id": "8MhL3HVUK0Ia"
      },
      "id": "8MhL3HVUK0Ia",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Translation"
      ],
      "metadata": {
        "id": "nSV7C6F7Qyoy"
      },
      "id": "nSV7C6F7Qyoy"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Pipeline\n",
        " \n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"Helsinki-NLP/opus-mt-en-fr\")\n",
        "\n",
        "pipe = pipeline(task=\"translation\", \n",
        "                model=model,\n",
        "                tokenizer=tokenizer)\n",
        "\n",
        "def translate(pipe, prompt):\n",
        "    print(f\"\\nOriginal :\\n{prompt}\")\n",
        "    translation = pipe(prompt)\n",
        "    print(f\"Translation : \\n{translation[0]['translation_text']}\")\n",
        "\n",
        "translate(pipe, 'Paris is a nice and friendly city.')"
      ],
      "metadata": {
        "id": "1mJlquLnMVf5"
      },
      "id": "1mJlquLnMVf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Questioning & Answering"
      ],
      "metadata": {
        "id": "FB68q7UuUBzF"
      },
      "id": "FB68q7UuUBzF"
    },
    {
      "cell_type": "code",
      "source": [
        "context  = r\"\"\"\n",
        "The Mars Orbiter Mission (MOM), also called Mangalyaan (\"Mars-craft\", from\n",
        "Mangala, \"Mars\" and yāna, \"craft, vehicle\") is a space probe orbiting Mars since\n",
        "24 September 2014? It was launched on 5 November 2013 by the Indian Space\n",
        "Research Organisation (ISRO). It is India's first interplanetary mission and it\n",
        "made India the fourth country to achieve Mars orbit, after Roscosmos, NASA, and\n",
        "the European Space Company. and it made India the first country to achieve this\n",
        "in the first attempt. The Mars Orbiter took off from the First Launch Pad at\n",
        "Satish Dhawan Space Centre (Sriharikota Range SHAR), Andhra Pradesh, using a\n",
        "Polar Satellite Launch Vehicle (PSLV) rocket C25 at 09:08 UTC on 5 November\n",
        "2013. The launch window was approximately 20 days long and started on 28 October\n",
        "2013. The MOM probe spent about 36 days in  Earth orbit, where it made a series\n",
        "of seven apogee-raising orbital maneuvers before trans-Mars injection on 30 \n",
        "November 2013 (UTC).[23] After a 298-day long journey to Mars orbit, it was put\n",
        "into Mars orbit on 24 September 2014.\n",
        "\"\"\"\n",
        "\n",
        "pipe = pipeline(task=\"question-answering\",\n",
        "               model=\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "def answer_question(pipe, question, context):\n",
        "    print(f\"\\nQuestion : {question}\")\n",
        "    answer = pipe(question=question, context=context)\n",
        "    print(f\"Answer    : {answer['answer']}\")\n",
        "\n",
        "answer_question(pipe, \"When did Mars Mission Launched?\", context)\n"
      ],
      "metadata": {
        "id": "PF0osm9kRrIo"
      },
      "id": "PF0osm9kRrIo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Token classification (Named Entity Recognition)"
      ],
      "metadata": {
        "id": "3PJ-ZtWNWAqm"
      },
      "id": "3PJ-ZtWNWAqm"
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline(task=\"ner\",\n",
        "                model=\"dbmdz/bert-large-cased-finetuned-conll03-english\")\n",
        "\n",
        "def classify(pipe, prompt):\n",
        "    print(f\"\\nPrompt : {prompt}\")\n",
        "    answer = pipe(prompt)\n",
        "    print(f\"Entities: \")\n",
        "    for ent in answer:\n",
        "      print(f\"{ent['entity']} : {ent['word']}\")\n",
        "\n",
        "classify(pipe, \"Hello I'm Leo and I live in Utrecht.\")\n"
      ],
      "metadata": {
        "id": "Ac0tBzmrU0LU"
      },
      "id": "Ac0tBzmrU0LU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fill masked word"
      ],
      "metadata": {
        "id": "6GgH-HH4YGsj"
      },
      "id": "6GgH-HH4YGsj"
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipeline('fill-mask', model='bert-large-uncased-whole-word-masking')\n",
        "\n",
        "def fill_mask(pipe, prompt):\n",
        "    print(f\"\\nPrompt : {prompt}\")\n",
        "    answer = pipe(prompt)\n",
        "    print(f\"Answer : {answer[0]['sequence']}\")\n",
        "    print(f\"Score  : {answer[0]['score']}\")\n",
        "\n",
        "fill_mask(pipe, \"Hello I'm a [MASK] model.\")"
      ],
      "metadata": {
        "id": "8wZEppEtWMDi"
      },
      "id": "8wZEppEtWMDi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text generation"
      ],
      "metadata": {
        "id": "maKJ-JDBySN7"
      },
      "id": "maKJ-JDBySN7"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "pipe = pipeline('text-generation', model = 'gpt2')\n",
        "\n",
        "def generate_text(pipe, prompt, max_length = 30, num_return_sequences=3):\n",
        "  print(f\"\\nPrompt : {prompt}\")\n",
        "  gen_text = pipe(\"Hello, I am presenting on\", max_length=max_length, \n",
        "                       num_return_sequences=num_return_sequences)\n",
        "  print(f\"Results:\")\n",
        "  for r in gen_text:\n",
        "    print(\"------------------------------------------------------------\")\n",
        "    print(f\"{r['generated_text']}\")\n",
        "    print(\"------------------------------------------------------------\")\n",
        "generate_text(pipe, \"Hello, I am presenting on\")\n"
      ],
      "metadata": {
        "id": "nGUkI7yCaATg"
      },
      "id": "nGUkI7yCaATg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y7CodRpgz1TU"
      },
      "id": "Y7CodRpgz1TU",
      "execution_count": 26,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}